version: '3'
services:
  namenode:
    image: apache/hadoop:3.3.5
    container_name: namenode
    hostname: namenode
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - ./hadoop_namenode:/opt/hadoop/data/nameNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop
      - ./start-hdfs.sh:/start-hdfs.sh
    command: [ "/bin/bash", "/start-hdfs.sh" ]
    networks:
      - hadoop

  datanode1:
    image: apache/hadoop:3.3.5
    container_name: datanode1
    hostname: datanode1
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - ./hadoop_datanode1:/opt/hadoop/data/dataNode
      - ./hadoop_config:/opt/hadoop/etc/hadoop
      - ./init-datanode.sh:/init-datanode.sh
    depends_on:
      - namenode
    command: [ "/bin/bash", "/init-datanode.sh" ]
    networks:
      - hadoop

  dataprocessing:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: dataprocessing
    hostname: dataprocessing
    depends_on:
      - namenode
      - datanode1
    volumes:
      - .:/opt/spark-app
    entrypoint: [ "/opt/spark/bin/spark-submit", "/opt/spark-app/data_processing.py" ]
    networks:
      - hadoop

  mlmodel:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: mlmodel
    hostname: mlmodel
    depends_on:
      - datanode1
      - dataprocessing
    volumes:
      - .:/opt/spark-app
    entrypoint: [ "sh", "-c", "sleep 60 && /opt/spark/bin/spark-submit /opt/spark-app/ml_model.py" ]
    networks:
      - hadoop

networks:
  hadoop:
    driver: bridge
